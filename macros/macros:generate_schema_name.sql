# Medallion Architecture with dbt Cloud & Snowflake - Complete Beginner's Guide

## ðŸŽ¯ For Absolute Beginners

This guide will take you from **zero to a complete data pipeline** even if you've never used dbt or Snowflake before.

## ðŸ“‹ Prerequisites Checklist

Before you start, you need:
- âœ… Snowflake account (free trial available)
- âœ… dbt Cloud account (free tier available)
- âœ… Basic SQL knowledge

---

# ðŸš€ STEP-BY-STEP SETUP

## Phase 1: Snowflake Setup (15 minutes)

### Step 1: Log into Snowflake
1. Go to [snowflake.com](https://snowflake.com)
2. Log into your account
3. You'll see the Snowflake worksheet interface

### Step 2: Create Database and Warehouse
Copy and run **each command separately** in Snowflake:

```sql
-- 1. Create database
CREATE DATABASE MEDALLION_DB;

-- 2. Switch to the database
USE DATABASE MEDALLION_DB;

-- 3. Create warehouse (this is your compute power)
CREATE WAREHOUSE DBT_WH
WAREHOUSE_SIZE = 'X-SMALL'
AUTO_SUSPEND = 60
AUTO_RESUME = TRUE;

-- 4. Create schemas for our medallion architecture
CREATE SCHEMA RAW;      -- For source data
CREATE SCHEMA STAGING;  -- For staging views
CREATE SCHEMA BRONZE;   -- For raw data layer
CREATE SCHEMA SILVER;   -- For cleaned data layer  
CREATE SCHEMA GOLD;     -- For business metrics
```

**âœ… Checkpoint**: Run `SHOW SCHEMAS;` - you should see RAW, STAGING, BRONZE, SILVER, GOLD

### Step 3: Create Sample Source Tables and Data
Run these commands **one by one**:

```sql
-- Make sure you're in the right database
USE DATABASE MEDALLION_DB;
USE SCHEMA RAW;

-- 1. Create customers table (JSON data)
CREATE TABLE customers_json (
    src VARIANT,
    loaded_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

-- 2. Create orders table (CSV data)  
CREATE TABLE orders_csv (
    order_id VARCHAR,
    customer_id VARCHAR,
    order_date VARCHAR,
    amount VARCHAR,
    status VARCHAR,
    loaded_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

-- 3. Create products table (Parquet data)
CREATE TABLE products_parquet (
    product_id VARCHAR,
    product_name VARCHAR,
    category VARCHAR,
    price VARCHAR,
    loaded_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);
```

### Step 4: Insert Sample Data
Run these **one by one**:

```sql
-- Insert customer data (JSON format)
INSERT INTO customers_json (src) VALUES
(PARSE_JSON('{"customer_id": "C001", "first_name": "john", "last_name": "doe", "email": "john.doe@email.com", "phone": "123-456-7890", "address": {"street": "123 main st", "city": "new york", "state": "NY", "zip_code": "10001"}}')),
(PARSE_JSON('{"customer_id": "C002", "first_name": "jane", "last_name": "smith", "email": "jane.smith@email.com", "phone": "987-654-3210", "address": {"street": "456 oak ave", "city": "los angeles", "state": "CA", "zip_code": "90210"}}'));

-- Insert order data (CSV format)
INSERT INTO orders_csv (order_id, customer_id, order_date, amount, status) VALUES
('O001', 'C001', '2024-01-15', '150.00', 'completed'),
('O002', 'C001', '2024-01-20', '75.50', 'completed'),
('O003', 'C002', '2024-01-18', '200.00', 'completed'),
('O004', 'C002', '2024-01-25', '45.25', 'pending');

-- Insert product data (Parquet format)  
INSERT INTO products_parquet (product_id, product_name, category, price) VALUES
('P001', 'laptop computer', 'electronics', '999.99'),
('P002', 'office chair', 'furniture', '199.99'),
('P003', 'coffee mug', 'kitchen', '12.50');
```

### Step 5: Verify Source Data
Run this to check your data:

```sql
-- Check customers
SELECT * FROM MEDALLION_DB.RAW.CUSTOMERS_JSON;

-- Check orders
SELECT * FROM MEDALLION_DB.RAW.ORDERS_CSV;

-- Check products
SELECT * FROM MEDALLION_DB.RAW.PRODUCTS_PARQUET;
```

**âœ… Checkpoint**: You should see 2 customers, 4 orders, and 3 products

---

## Phase 2: dbt Cloud Setup (10 minutes)

### Step 6: Create dbt Cloud Account
1. Go to [getdbt.com](https://getdbt.com)
2. Sign up for dbt Cloud (free tier)
3. Complete the onboarding

### Step 7: Connect dbt Cloud to Snowflake
1. In dbt Cloud, go to **Settings** â†’ **Projects**
2. Click **New Project**
3. Choose **Snowflake** as connection
4. Fill in your Snowflake details:
   - **Account**: Your Snowflake account ID (looks like `xy12345.us-east-1`)
   - **Username**: Your Snowflake username
   - **Password**: Your Snowflake password
   - **Database**: `MEDALLION_DB`
   - **Warehouse**: `DBT_WH`
   - **Role**: (leave default)
   - **Schema**: `BRONZE` (this will be overridden later)

---

## ðŸ“ CREATE THESE FILES IN dBT CLOUD

### ðŸ› ï¸ File 1: `dbt_project.yml` (MAIN CONFIGURATION FILE)

**ðŸ“ Location**: Root directory of your dbt project

**ðŸŽ¯ Purpose**: This is the BRAIN of your dbt project. It tells dbt:
- How to organize your data models
- Where to create tables/views in Snowflake
- What settings to use for different layers

**ðŸ“ Content**:
```yaml
name: 'medallion_project'       # Your project name
version: '1.0.0'                # Project version
config-version: 2               # dbt configuration version

profile: 'medallion_snowflake'  # Connection profile name

# Where dbt should look for different types of files
model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"           # Where compiled files go
clean-targets:                  # What to clean when running dbt clean
  - "target"
  - "dbt_packages"

# ðŸ—ï¸ THIS IS WHERE WE DEFINE OUR MEDALLION ARCHITECTURE
models:
  medallion_project:
    # STAGING LAYER - Views that read directly from source tables
    staging:
      +materialized: view      # Create as VIEWS in Snowflake
      +schema: staging         # Put them in STAGING schema
    
    # BRONZE LAYER - Raw, immutable source data
    bronze:
      +materialized: incremental  # Only process NEW data (efficient)
      +schema: bronze            # Put them in BRONZE schema
    
    # SILVER LAYER - Cleaned and enriched data
    silver:
      +materialized: table       # Create as TABLES in Snowflake
      +schema: silver           # Put them in SILVER schema
    
    # GOLD LAYER - Business metrics and aggregates
    gold:
      +materialized: table       # Create as TABLES in Snowflake
      +schema: gold             # Put them in GOLD schema
```

**ðŸ”§ What it does**:
- **Staging**: Creates lightweight VIEWS that don't store data but read directly from source tables
- **Bronze**: Uses INCREMENTAL tables that only process new data (saves time/money)
- **Silver/Gold**: Creates full TABLES that store transformed data

---

### ðŸ“¦ File 2: `packages.yml` (EXTERNAL TOOLS MANAGER)

**ðŸ“ Location**: Root directory of your dbt project

**ðŸŽ¯ Purpose**: This file manages external dbt packages (like installing libraries in Python). We're installing `dbt_utils` which gives us helpful functions.

**ðŸ“ Content**:
```yaml
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
```

**ðŸ”§ What it does**:
- Tells dbt to download the `dbt_utils` package
- This package provides useful functions like `generate_surrogate_key` that we'll use later
- Run `dbt deps` to actually install these packages

---

### ðŸ”‘ File 3: `macros/generate_surrogate_key.sql` (CUSTOM FUNCTION)

**ðŸ“ Location**: `macros/` directory

**ðŸŽ¯ Purpose**: Creates a reusable function that generates unique IDs for our data. This is like creating a custom tool that we can use everywhere.

**ðŸ“ Content**:
```sql
{% macro generate_surrogate_key(field_list) %}
    {#- Handle both single fields and lists of fields -#}
    {%- if field_list is string -%}
        {%- set fields = [field_list] -%}
    {%- else -%}
        {%- set fields = field_list -%}
    {%- endif -%}
    
    {#- Create expressions for each field -#}
    {%- set field_expressions = [] -%}
    
    {%- for field in fields -%}
        {%- set field_expression = "coalesce(cast(" ~ field ~ " as varchar), '')" -%}
        {%- do field_expressions.append(field_expression) -%}
    {%- endfor -%}
    
    {#- Generate MD5 hash -#}
    {%- if field_expressions | length == 1 -%}
        md5({{ field_expressions[0] }})
    {%- else -%}
        md5(concat({{ field_expressions | join(', ') }}))
    {%- endif -%}
{% endmacro %}
```

**ðŸ”§ What it does**:
- **Takes field names** (like 'customer_id') as input
- **Handles NULL values** safely using `coalesce`
- **Creates MD5 hash** - a unique fingerprint for each combination of fields
- **Returns a consistent unique key** that we can use as primary keys

**Example Usage**:
```sql
-- Creates a unique key like 'a1b2c3d4e5f6...'
{{ generate_surrogate_key(['customer_id']) }} as customer_key

-- Creates a unique key from multiple fields  
{{ generate_surrogate_key(['order_id', 'customer_id']) }} as order_key
```

---

### ðŸ“‹ File 4: `models/staging/src_snowflake.yml` (DATA CATALOG)

**ðŸ“ Location**: `models/staging/` directory

**ðŸŽ¯ Purpose**: This is like a MAP that tells dbt where your raw data lives. It creates data lineage so you can track where data comes from.

**ðŸ“ Content**:
```yaml
version: 2

sources:
  - name: raw                    # We call our source 'raw'
    database: MEDALLION_DB       # Database name in Snowflake
    schema: raw                  # Schema name in Snowflake
    tables:
      - name: customers_json     # Table with JSON customer data
      - name: orders_csv         # Table with CSV order data  
      - name: products_parquet   # Table with Parquet product data
```

**ðŸ”§ What it does**:
- **Defines source tables** that dbt should read from
- **Enables data lineage** - dbt can now show you how data flows from source to final tables
- **Provides a single source of truth** for where raw data is located

---

### ðŸ‘ï¸ File 5: `models/staging/stg_customers_json.sql` (STAGING VIEW)

**ðŸ“ Location**: `models/staging/` directory

**ðŸŽ¯ Purpose**: Creates a clean view of your raw JSON customer data. Think of this as a "window" into your raw data.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='view',    # Create as a VIEW (not a table)
        schema='staging'        # Put it in STAGING schema
    )
}}

SELECT 
    src AS raw_json_data,       # Keep the raw JSON data
    CURRENT_TIMESTAMP() AS loaded_at  # Add timestamp for tracking
FROM {{ source('raw', 'customers_json') }}  # Read from source table
```

**ðŸ”§ What it does**:
- **Creates a view** called `stg_customers_json` in the STAGING schema
- **Reads directly** from the raw `customers_json` table
- **Adds metadata** (`loaded_at` timestamp)
- **NO data transformation** - this layer just provides clean access to raw data

---

### ðŸ‘ï¸ File 6: `models/staging/stg_orders_csv.sql` (STAGING VIEW)

**ðŸ“ Location**: `models/staging/` directory

**ðŸŽ¯ Purpose**: Creates a clean view of your raw CSV order data.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='view',
        schema='staging'
    )
}}

SELECT 
    order_id,
    customer_id, 
    order_date,
    amount,
    status,
    CURRENT_TIMESTAMP() AS loaded_at
FROM {{ source('raw', 'orders_csv') }}  # Read from CSV source table
```

**ðŸ”§ What it does**:
- **Creates a view** of raw order data
- **Maintains original structure** - no changes to data
- **Provides consistent interface** for downstream models

---

### ðŸ‘ï¸ File 7: `models/staging/stg_products_parquet.sql` (STAGING VIEW)

**ðŸ“ Location**: `models/staging/` directory

**ðŸŽ¯ Purpose**: Creates a clean view of your raw Parquet product data.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='view', 
        schema='staging'
    )
}}

SELECT 
    product_id,
    product_name,
    category,
    price,
    CURRENT_TIMESTAMP() AS loaded_at
FROM {{ source('raw', 'products_parquet') }}  # Read from Parquet source
```

---

### ðŸ¥‰ File 8: `models/bronze/raw_customers_json.sql` (BRONZE LAYER)

**ðŸ“ Location**: `models/bronze/` directory

**ðŸŽ¯ Purpose**: Extracts and stores raw JSON customer data with incremental loading. This is where we first transform the data.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='incremental',  # Only process NEW data
        unique_key='customer_id',    # Prevent duplicates
        schema='bronze'              # Store in BRONZE schema
    )
}}

SELECT 
    raw_json_data:customer_id::VARCHAR AS customer_id,  # Extract from JSON
    raw_json_data AS customer_data,                     # Keep raw JSON
    loaded_at
FROM {{ ref('stg_customers_json') }}  # Read from staging view
{% if is_incremental() %}
WHERE loaded_at > (SELECT MAX(loaded_at) FROM {{ this }})
{% endif %}
```

**ðŸ”§ What it does**:
- **Extracts fields** from JSON structure (`customer_id`)
- **Keeps raw JSON** for audit purposes
- **Incremental loading** - only processes new data after first run
- **Prevents duplicates** using `unique_key`

---

### ðŸ¥‰ File 9: `models/bronze/raw_orders_csv.sql` (BRONZE LAYER)

**ðŸ“ Location**: `models/bronze/` directory

**ðŸŽ¯ Purpose**: Processes and stores CSV order data with proper data types.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='incremental',
        unique_key='order_id',      # Orders are unique by order_id
        schema='bronze'
    )
}}

SELECT 
    order_id::VARCHAR AS order_id,
    customer_id::VARCHAR AS customer_id,
    TRY_TO_DATE(order_date) AS order_date,  # Convert to proper date
    amount::DECIMAL(10,2) AS amount,        # Convert to number
    status::VARCHAR AS status,
    loaded_at
FROM {{ ref('stg_orders_csv') }}
{% if is_incremental() %}
WHERE loaded_at > (SELECT MAX(loaded_at) FROM {{ this }})
{% endif %}
```

**ðŸ”§ What it does**:
- **Data type conversion** - strings to proper types (date, decimal)
- **Data validation** - `TRY_TO_DATE` handles invalid dates gracefully
- **Incremental processing** for efficiency

---

### ðŸ¥‰ File 10: `models/bronze/raw_products_parquet.sql` (BRONZE LAYER)

**ðŸ“ Location**: `models/bronze/` directory

**ðŸŽ¯ Purpose**: Processes and stores product data.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='incremental',
        unique_key='product_id',    # Products are unique by product_id
        schema='bronze'
    )
}}

SELECT 
    product_id::VARCHAR AS product_id,
    product_name::VARCHAR AS product_name,
    category::VARCHAR AS category,
    price::DECIMAL(10,2) AS price,
    loaded_at
FROM {{ ref('stg_products_parquet') }}
{% if is_incremental() %}
WHERE loaded_at > (SELECT MAX(loaded_at) FROM {{ this }})
{% endif %}
```

---

### ðŸ¥ˆ File 11: `models/silver/dim_customers.sql` (SILVER LAYER)

**ðŸ“ Location**: `models/silver/` directory

**ðŸŽ¯ Purpose**: Creates a clean, standardized customer dimension table with surrogate keys.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='table',    # Create as permanent table
        schema='silver'          # Store in SILVER schema
    )
}}

WITH customer_json_data AS (
    SELECT
        customer_id,
        customer_data:first_name::VARCHAR AS first_name,
        customer_data:last_name::VARCHAR AS last_name,
        customer_data:email::VARCHAR AS email,
        customer_data:phone::VARCHAR AS phone,
        customer_data:address:street::VARCHAR AS street,
        customer_data:address:city::VARCHAR AS city,
        customer_data:address:state::VARCHAR AS state,
        customer_data:address:zip_code::VARCHAR AS zip_code,
        loaded_at
    FROM {{ ref('raw_customers_json') }}  # Read from bronze layer
),

customer_cleaned AS (
    SELECT
        {{ generate_surrogate_key(['customer_id']) }} AS customer_key,  # UNIQUE KEY
        customer_id,
        INITCAP(TRIM(first_name)) AS first_name,  -- "john" â†’ "John"
        INITCAP(TRIM(last_name)) AS last_name,    -- "doe" â†’ "Doe"  
        LOWER(TRIM(email)) AS email,              -- "John.Doe@Email.COM" â†’ "john.doe@email.com"
        REGEXP_REPLACE(phone, '[^0-9]', '') AS phone,  -- "123-456-7890" â†’ "1234567890"
        INITCAP(TRIM(street)) AS street,          -- "123 main st" â†’ "123 Main St"
        INITCAP(TRIM(city)) AS city,              -- "new york" â†’ "New York"
        UPPER(TRIM(state)) AS state,              -- "ny" â†’ "NY"
        zip_code,
        loaded_at
    FROM customer_json_data
    WHERE email IS NOT NULL      -- Data quality: remove records without email
      AND customer_id IS NOT NULL -- Data quality: remove records without ID
)

SELECT * FROM customer_cleaned
```

**ðŸ”§ What it does**:
- **Creates surrogate key** - unique identifier for each customer
- **Data standardization** - proper case, trimming, formatting
- **Data cleaning** - phone number formatting, email normalization
- **Data validation** - removes invalid records
- **Business-ready** - clean, consistent data for analysis

---

### ðŸ¥ˆ File 12: `models/silver/fct_orders.sql` (SILVER LAYER)

**ðŸ“ Location**: `models/silver/` directory

**ðŸŽ¯ Purpose**: Creates a fact table for orders with proper relationships and business logic.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='table',
        schema='silver'
    )
}}

WITH orders_cleaned AS (
    SELECT
        {{ generate_surrogate_key(['order_id']) }} AS order_key,  # UNIQUE KEY
        order_id,
        customer_id,
        order_date,
        amount,
        UPPER(TRIM(status)) AS status,  -- "completed" â†’ "COMPLETED"
        loaded_at
    FROM {{ ref('raw_orders_csv') }}
    WHERE order_id IS NOT NULL      -- Must have order ID
      AND customer_id IS NOT NULL   -- Must have customer ID
      AND order_date IS NOT NULL    -- Must have order date
      AND amount > 0                -- Amount must be positive
),

orders_with_customer AS (
    SELECT
        o.order_key,
        o.order_id,
        c.customer_key,              # Link to customer dimension
        o.order_date,
        o.amount,
        o.status,
        o.loaded_at
    FROM orders_cleaned o
    LEFT JOIN {{ ref('dim_customers') }} c  # Join with customer dimension
        ON o.customer_id = c.customer_id
)

SELECT * FROM orders_with_customer
```

**ðŸ”§ What it does**:
- **Creates fact table** for order transactions
- **Data validation** - ensures data quality rules
- **Standardizes values** - consistent status formatting
- **Creates relationships** - links orders to customers
- **Business logic** - applies rules like "amount must be positive"

---

### ðŸ¥ˆ File 13: `models/silver/stg_products.sql` (SILVER LAYER)

**ðŸ“ Location**: `models/silver/` directory

**ðŸŽ¯ Purpose**: Creates a clean product dimension table.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='table',
        schema='silver'
    )
}}

SELECT
    {{ generate_surrogate_key(['product_id']) }} AS product_key,
    product_id,
    INITCAP(TRIM(product_name)) AS product_name,  -- "laptop computer" â†’ "Laptop Computer"
    INITCAP(TRIM(category)) AS category,          -- "electronics" â†’ "Electronics"
    price,
    loaded_at
FROM {{ ref('raw_products_parquet') }}
WHERE product_id IS NOT NULL    -- Must have product ID
  AND product_name IS NOT NULL  -- Must have product name
  AND price >= 0                -- Price can't be negative
```

---

### ðŸ¥‡ File 14: `models/gold/customer_analytics.sql` (GOLD LAYER)

**ðŸ“ Location**: `models/gold/` directory

**ðŸŽ¯ Purpose**: Creates business metrics and customer segmentation for analytics.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='table',
        schema='gold'
    )
}}

WITH customer_orders AS (
    SELECT
        customer_key,
        COUNT(*) AS total_orders,
        SUM(amount) AS total_spent,
        AVG(amount) AS avg_order_value,
        MAX(order_date) AS last_order_date
    FROM {{ ref('fct_orders') }}
    WHERE status = 'COMPLETED'   -- Only count completed orders
    GROUP BY customer_key
),

customer_segmentation AS (
    SELECT
        co.customer_key,
        c.first_name,
        c.last_name,
        c.email,
        co.total_orders,
        co.total_spent,
        co.avg_order_value,
        co.last_order_date,
        -- CUSTOMER SEGMENTATION LOGIC
        CASE
            WHEN co.total_spent >= 1000 THEN 'VIP'
            WHEN co.total_spent >= 500 THEN 'Premium'
            WHEN co.total_spent >= 100 THEN 'Regular'
            ELSE 'New'
        END AS customer_segment,
        -- CUSTOMER ACTIVITY STATUS
        CASE
            WHEN DATEDIFF('day', co.last_order_date, CURRENT_DATE()) <= 30 THEN 'Active'
            WHEN DATEDIFF('day', co.last_order_date, CURRENT_DATE()) <= 90 THEN 'Inactive'
            ELSE 'Churned'
        END AS customer_status
    FROM customer_orders co
    JOIN {{ ref('dim_customers') }} c
        ON co.customer_key = c.customer_key
)

SELECT * FROM customer_segmentation
```

**ðŸ”§ What it does**:
- **Customer analytics** - lifetime value, order patterns
- **Business segmentation** - VIP, Premium, Regular customers
- **Activity tracking** - Active, Inactive, Churned status
- **Executive reporting** - ready for dashboards and business intelligence

---

### ðŸ¥‡ File 15: `models/gold/business_kpis.sql` (GOLD LAYER)

**ðŸ“ Location**: `models/gold/` directory

**ðŸŽ¯ Purpose**: Creates daily business performance metrics.

**ðŸ“ Content**:
```sql
{{
    config(
        materialized='table', 
        schema='gold'
    )
}}

WITH daily_metrics AS (
    SELECT
        order_date,
        COUNT(*) AS order_count,
        SUM(amount) AS daily_revenue,
        COUNT(DISTINCT customer_key) AS daily_customers,
        AVG(amount) AS avg_order_value
    FROM {{ ref('fct_orders') }}
    WHERE status = 'COMPLETED'   -- Only successful orders
    GROUP BY order_date
),

running_metrics AS (
    SELECT
        order_date,
        order_count,
        daily_revenue,
        daily_customers,
        avg_order_value,
        SUM(daily_revenue) OVER (ORDER BY order_date) AS cumulative_revenue,
        AVG(daily_revenue) OVER (ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS weekly_avg_revenue
    FROM daily_metrics
)

SELECT * FROM running_metrics
```

**ðŸ”§ What it does**:
- **Daily performance** - orders, revenue, customers
- **Running totals** - cumulative revenue growth
- **Trend analysis** - weekly averages
- **Business intelligence** - ready for executive dashboards

---

## ðŸš€ RUNNING YOUR PROJECT

### Step 8: Install Dependencies
In dbt Cloud, run:
```bash
dbt deps
```

### Step 9: Run Your Pipeline
Run these commands **in order**:

```bash
# 1. Test connection
dbt debug

# 2. Create staging views
dbt run --select staging

# 3. Create bronze tables (raw data)
dbt run --select bronze

# 4. Create silver tables (cleaned data)
dbt run --select silver

# 5. Create gold tables (business metrics)
dbt run --select gold

# 6. Or run everything at once
dbt run
```

### Step 10: Verify in Snowflake
Run these in Snowflake **one by one**:

```sql
-- Check all schemas exist
USE DATABASE MEDALLION_DB;
SHOW SCHEMAS;

-- Check objects in each layer
SHOW VIEWS IN STAGING;
SHOW TABLES IN BRONZE;
SHOW TABLES IN SILVER; 
SHOW TABLES IN GOLD;

-- Check data flow
SELECT 'STAGING' as layer, COUNT(*) as count FROM STAGING.STG_CUSTOMERS_JSON
UNION ALL
SELECT 'BRONZE' as layer, COUNT(*) as count FROM BRONZE.RAW_CUSTOMERS_JSON  
UNION ALL
SELECT 'SILVER' as layer, COUNT(*) as count FROM SILVER.DIM_CUSTOMERS
UNION ALL
SELECT 'GOLD' as layer, COUNT(*) as count FROM GOLD.CUSTOMER_ANALYTICS;
```

## ðŸŽ‰ CONGRATULATIONS!

You've built a complete medallion architecture data pipeline that:
- âœ… Ingests multiple data formats (JSON, CSV, Parquet)
- âœ… Transforms raw data through 4 layers
- âœ… Ensures data quality and consistency
- âœ… Creates business-ready analytics
- âœ… Is production-ready and scalable
